{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Notebook 4: Descriptive Statistics\n",
    "\n",
    "*Developed by Johannes Haas and Raoul Collenteur, Institute of Earth Sciences, NAWI Graz Geocenter, University of Graz.*\n",
    "\n",
    "### Content of this lecture\n",
    "- repetition of previous lecture\n",
    "- some pointers on file types\n",
    "- basic statistics\n",
    "- correlations\n",
    "- fitting linear functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the python packages needed in this session\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Repeating last week\n",
    "### 1a. Functions\n",
    "Last week we looked into functions, reusable sequences of Python code that can be used to perform a certain task multiple times. Let's look at these once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply2(a, b=2):\n",
    "    val = a * b\n",
    "    return val\n",
    "\n",
    "print(multiply2(5)) # No need to fill in the value b\n",
    "print(multiply2(5, 3)) # But we still can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Reading CSV files\n",
    "\n",
    "There are many ways to read CSV files. The most widely used and most important one for our purposes is `read_csv()` from the [pandas](http://pandas.pydata.org/) package.\n",
    "`CSV` stands for *comma separated value(s)*, meaning that your csv file is just a text file (`.txt`) with a comma separating the values. E.g.:\n",
    "\n",
    "    apples,  USD, 5.99\n",
    "    oranges, USD, 7.99\n",
    "    bananas, USD, 3.99\n",
    "\n",
    "However, keep in mind that this, like most things in IT, follows US standards!\n",
    "For Austria, where we are using the `,` as a decimal point, using the comma also as a separator would cause confusion:\n",
    "\n",
    "    Ã„pfel,   EUR, 5,36\n",
    "    \n",
    "So in Austria, the semicolon `;` tends to get used as the separator.\n",
    "Since the csv file is just a text file, this does not really matter, however, you have to remember that you have to specify these things, using the `sep=` and `decimal=` options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Some pointers on file types\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Common file types\n",
    "\n",
    "If you look around on your computer, you'll find various files, generally marked by a symbol/icon and their file extension. E.g a page of paper with a \"W\" and the ending `.doc` or `.docx` is a word file, if that \"W\" is an \"X\" and it ends with `.xls` or `.xlsx`, it's an excel file, and so on.\n",
    "\n",
    "So normally, you just doubleclick a file, and windows knows what to do with it, and thanks to the icon, you know what's likely going to happen (\"W\" indicates that word will start...). However, this system has some limitations:\n",
    "When you change the file ending, you don't necessarily change the file. And icons that point to specific applications can hide the fact that there's also other applications that can open said files.\n",
    "And as you have already realized, windows sometimes gets confused when saving files from the internet or when opening files it does not know. E.g. when Edge tries to save our notebooks as `.htm` or when you try to open our `.ipynb` files with a simple doubleclick.\n",
    "So this simple principle offers some pitfalls, when you venture away from the small bunch of standard MS-Office files or when you are working with files that can be opened with many different applications.\n",
    "\n",
    "There are various ways to improve this situation such as \"[magic numbers](https://en.wikipedia.org/wiki/Magic_number_(programming))\" and the use of metadata. However, for our use case, the file extension and a bit of thinking will work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. text files\n",
    "\n",
    "Most of the files we are using are just simple text files.\n",
    "The CSV files from the previous lecture, are nothing more than a text file (`.txt`) with a different file extension (`.csv`) and a few rules for its formatting.\n",
    "Similarly, the jupyter notebook is also just a simple text file, but with the ending `.ipynb` and certain rules that govern its structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    {\n",
    "     \"cells\": [\n",
    "      {\n",
    "       \"cell_type\": \"markdown\",\n",
    "       \"metadata\": {},\n",
    "       \"source\": [\n",
    "        \"\\n\",\n",
    "        \"# Notebook 4: Descriptive Statistics\\n\",\n",
    "        \"\\n\",\n",
    "        \"*Developed by Johannes Haas and Raoul Collenteur, Institute of Earth Sciences, NAWI Graz Geocenter, University of Graz.*\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Content of this lecture\\n\",\n",
    "        \"- repetition of previous lecture\\n\",\n",
    "        \"- some pointers on file types\\n\",\n",
    "        \"- basic statistics\\n\",\n",
    "        \"- correlations\\n\",\n",
    "        \"- fitting linear functions\\n\",\n",
    "        \"\\n\"\n",
    "       ]\n",
    "      },\n",
    "      {\n",
    "       \"cell_type\": \"code\",\n",
    "       \"execution_count\": 2,\n",
    "       \"metadata\": {},\n",
    "       \"outputs\": [],\n",
    "       \"source\": [\n",
    "        \"# Import the python packages needed in this session\\n\",\n",
    "        \"import numpy as np\\n\",\n",
    "        \"import pandas as pd\\n\",\n",
    "        \"import matplotlib.pyplot as plt\\n\",\n",
    "        \"%matplotlib inline\"\n",
    "       ]\n",
    "      },"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *code* above is just the first two cells of this notebook, opened in a text editor.\n",
    "You can probably already read quite a bit of this code, applying what you have learned so far, even though you probably don't know the *language* it is written in.\n",
    "\n",
    "For just editing the notebook, a text editor works just as well as our browser, but we have to deal with much more formatting commands and we can not work with the python parts interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Basic Statistics\n",
    "\n",
    "What basic statistics have we already done so far? Let us look at a few common statistics contained in the Numpy package. We first create an array of 100 random values using the `random` subpackage of Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first create a array of 100 random values between 0 and 1\n",
    "values = np.random.rand(100)\n",
    "print(\"The type of the variable 'values' is\", type(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print a couple of values in this array to see what we are dealing with\n",
    "values[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean of values is:\", np.mean(values))\n",
    "print(\"The standard deviation of values is:\", np.std(values))\n",
    "print(\"The minimum of values is:\", np.min(values))\n",
    "print(\"The maximum of values is:\", np.max(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two, very basic things are still missing: The median and the mode. \n",
    "\n",
    "### 3a. The median\n",
    "\n",
    "While the mean is the mean value of all the values, the median is the middle value of all sorted values.\n",
    "Or in other words: 50% of samples are smaller and 50% of samples are larger than the median.\n",
    "Lets construct some semi-random values to illustrate the point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICData = np.array([100, 200, 500, 500, 50, 250, 500, 600, 400, 100000,\n",
    "                   1000, 10, 250, 400, 600, 300, 500, 300, 500, 400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the mean is straightforward, both per hand, or with the inbuilt python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HandMean = sum(ICData) / len(ICData)\n",
    "PyMean = ICData.mean()\n",
    "print('The mean calculated the \\\"hard\" way is', HandMean, 'whereas the mean from the inbuilt method is', PyMean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the median is as easy, we just need a new function, `np.sort()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the values first:\n",
    "ICSorted = np.sort(ICData)\n",
    "# find the middle one\n",
    "middle = len(ICSorted) / 2\n",
    "middle = int(middle)\n",
    "median = ICSorted[middle]\n",
    "print(median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can simply use the inbuilt function, similar (but different) from our already known `mean` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('median:', np.median(ICSorted))\n",
    "print('mean:', ICSorted.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what's the difference between the two values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ICData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ICData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. The mode\n",
    "\n",
    "The mode (*Modalwert* or also *Modus* in German) is the value with the most occurences in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICSorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such a simple dataset, it's quite easy to count it by hand, but you can imagine that it will easily become a lot of work to implement an algorithm for it. \n",
    "We actually need another package for it, since it is not included in the standard numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = stats.mode(ICSorted)\n",
    "print(mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. The Bell Curve/Normal Distribution\n",
    "\n",
    "*Mostly called GauÃŸ-Verteilung in German*\n",
    "\n",
    "We have already worked with principles such as *mean* and the *standard deviation* that we applied to random values.\n",
    "However, most processes in nature are not really totally random, but follow certain probability density functions, with the most well known one being the [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) or [GauÃŸ-Verteilung](https://de.wikipedia.org/wiki/Normalverteilung) in German, with its characteristic bell shape.\n",
    "\n",
    "In the following code cells, we are going to produce such a dataset and figure out what this bell shape means.\n",
    "\n",
    "First, we have to produce a random dataset, that follows the normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GaussData = np.random.normal(10, 5, 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(GaussData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can clearly see, your data does not looks very bell-like.\n",
    "In order to turn it into the bell, we do not have to plot the values, but their counts!\n",
    "Again, `matplotlib`, which we have imported at the beginning, is there to help:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(GaussData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ypu can probably already already start to see some kind of bell shape. However, the bins we have used to sort our data have been decided by matplotlib, and they are not really the ideal size.\n",
    "Check out the documentation for `plt.hist()` (either with the `?` feature or [online](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html)) and replot the chart above with a bin size of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(-5,26,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(GaussData, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the data did not change, the distribution is likely a bit clearer now.\n",
    "Now we can also see what the `10` and `5` in our intial setup (`GaussData = np.random.normal(10, 5, 1000)`) meant.\n",
    "We have used the mean and the standard deviation as an input to produce a *random* normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = GaussData.mean()\n",
    "print('the mean is', mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = GaussData.std()\n",
    "print('the standard deviation is', sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation (mostly abbreviated with $\\sigma$) has a particular meaning for the normal distribution:\n",
    "about 68.3 % of all samples are within $\\pm \\sigma$ from the mean.\n",
    "\n",
    "### Excercise:\n",
    "\n",
    "Write a small script to figure out how many of our samples are indeed in the $\\pm \\sigma$ interval, using what you learned in the last lectures (loops and conditional statements):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: you can count things by initializing a counter and adding to it in a loop\n",
    "valueCounter = 0\n",
    "for number in GaussData:\n",
    "    valueCounter = valueCounter + 1\n",
    "print('valueCounter =', valueCounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Some more examples for probability distributions\n",
    "\n",
    "Our first example of the student incomes might be a tad extreme, but it does reflect the general distribution of incomes, where small and medium incomes tend to be the most common occurences, whereas high incomes are rarer, but can easily be extremely high.\n",
    "This is called a left skewed distribution (*linksschiefe Verteilung*).\n",
    "A right skewed distribution would be something where low and middle values tend to be rare, whereas the high values are the most likely occurence. \n",
    "A classic example for such a distribution would be the ages where people die (in wealthy countries).\n",
    "Infant mortality is quite low, and by far most kids survive their youth and teens and adults generally live a very safe live until they die from high age.\n",
    "\n",
    "The **normal distribution** we have plotted above is often relevant for the *results* of natural or machined processes.\n",
    "E.g. the distribution of biological traits (intelligence, height...) or manufacturing tolerances.\n",
    "\n",
    "The **gamma distribution** is relevant for hydrology, as most precipitation values tend to cluster around a (low) mode, but the distribution has a very long tail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for a gamma distribution\n",
    "s = np.random.gamma(2, 2, 1000)\n",
    "count, bins, ignored = plt.hist(s, 50, density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Correlations\n",
    "\n",
    "The correlation and thus the correlation coefficient is a measure for how similar two samples *behave*.\n",
    "[E.g if you notice a lot of storks in a town and you also notice that there are a lot of babies, the number of storks and the number of babies might be correlated.](http://www3.math.uni-paderborn.de/~agbiehler/sis/sisonline/struktur/jahrgang21-2001/heft2/Langfassungen/2001-2_Matth.pdf) Another example mentioned in this short paper is an obvious correlation between shoe sizes and reading ability.\n",
    "The bigger a shoe size, the better a person can read.\n",
    "So if you want to have a child, move to where the storks are and if you have issues reading these notebooks, buy bigger shoes...\n",
    "\n",
    "### 4a. Correlation != Causation\n",
    "\n",
    "While correlations can help to identify a causation, a high correlation alone is not a garantee for a causation.\n",
    "\n",
    "### 4b. Calculating the correlation\n",
    "\n",
    "Lets introduce some sample data from [this book](https://unikat.uni-graz.at:443/UGR:Gesamtbestand:UGR_alma21299366760003339)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([2, 4, 5, 6, 4, 7, 8, 5, 6, 7])\n",
    "Y = np.array([3, 2, 6, 5, 3, 6, 5, 4, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pearson correlation coefficient (a measurement for the correlation) generally called $r$, is calculated as follows:\n",
    "\n",
    "Eq.1 $$ r = \\frac{n \\sum{XY} - \\sum{X}\\sum{Y}}{\\sqrt{[n\\sum{X^2}-(\\sum{X})^2][n\\sum{Y^2} - (\\sum{Y})^2]}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this at first looks like a lot of work (and doing it by hand, it is), all of the needed sums are quite straightforward in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsum = sum(X)\n",
    "Ysum = sum(Y)\n",
    "Xsquared = X**2\n",
    "Ysquared = Y**2\n",
    "Xsqsum = sum(Xsquared)\n",
    "Ysqsum = sum(Ysquared)\n",
    "X_Y = X * Y\n",
    "X_Ysum = sum(X_Y)\n",
    "n = len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this could also be shortened by chaining functions. `Xsqsum` would also be correct with `Xsqsum = sum(X**2)`.\n",
    "With all the variables set, it should be a quite straightforward typing work.\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "Calculate r, following Equation 1 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: don't forget the correct positions of the brackets. \n",
    "# The sqaure brackets in the original formula are just for easy reading and don't work as normal brackets in python!\n",
    "r = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there is a shortcut in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. Correlating real data\n",
    "\n",
    "As you have learned last lecture, you can easily read csv data with pandas.\n",
    "Please download the groundwater levels (*Grundwasserstand-Monatsmittel*) for well `314922` and the water levels (*W-Tagesmittel*) for river gauge `211136` from [ehyd.gv.at](https://ehyd.gv.at) and save them as `well.csv` and `river.csv`.\n",
    "As you can clearly see, one is monthly data, whereas the other one is daily data, so we not only have to read them, but we also have to resample them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Since ehyd data has some very questionable format decisions, we have to give read_csv quite a lot of help to get started\n",
    "wellData = pd.read_csv('well.csv', sep = ';', skiprows=34, skipfooter=1, usecols=[0,1], index_col=0, parse_dates=True, dayfirst=True, names=['','level'], decimal = ',')\n",
    "riverData = pd.read_csv('river.csv', sep = ';', skiprows=23, skipfooter=1, index_col=0, parse_dates=True, dayfirst=True, names=['level'], decimal = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riverData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riverData.plot()\n",
    "wellData.plot(style='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=riverData.plot()\n",
    "wellData.plot(ax=ax, style='g', secondary_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to resample the data (=changing its time frequency), we first have to figure out how we want to resample it. So we check the heads of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wellData.head())\n",
    "print(riverData.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to resample our river data from daily data to monthly data, set to the beginning of the month.\n",
    "Also, for river data, we want a monthly average, and not a sum, so we have to feed some arguments to the resample function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riverMonthly = riverData.resample('MS').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riverMonthly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can calculate a correlation coefficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(riverMonthly, wellData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy correlation function only works for arrays of the same dimension. So we need to cut them to size and make sure to anly adress our data, and not the times.\n",
    "So we have to figure out the minimum and maximum times of the timeseries and cut them to that size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wellData.index.min())\n",
    "print(wellData.index.max())\n",
    "print(riverMonthly.index.min())\n",
    "print(riverMonthly.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellCut = wellData['1976':'2015']\n",
    "print(len(wellCut))\n",
    "print(len(riverMonthly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While they now have the same length, there are still some issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellCut.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to calculate a correlation coefficient, we need to turn the dataframe into a numpy array and reduce its dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_well = wellCut.values.squeeze()\n",
    "Y_river = riverMonthly.values.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(X_well,Y_river)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, getting this simple and important information was a lot of work.\n",
    "So you can probably already guess what's coming next:\n",
    "Of course there is a shortcut for it.\n",
    "We simply have to run the `corrwith()` function or we have to combine our dataframes and run the `.corr()` function. \n",
    "Pandas will even take care of the different lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the dataframes (we have to resample manually though) and tell pandas on which axis to concatenate\n",
    "corrDataframe = pd.concat([wellData, riverMonthly], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the correlation coefficient\n",
    "corrDataframe.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or, with keeping the dataframes as they are\n",
    "wellData.corrwith(riverMonthly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Exercises\n",
    "\n",
    "### Some basic statistics for real data\n",
    "\n",
    "Use pandas to obtain some basic statistics for `wellData` and `riverData`, so that you can **produce some output in the cell below:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The well Apfelberg (HZB 314922) has a mean waterlevel of', wellMean, 'meters, a median of', wellMedian, 'meters and a mode of', wellMode)\n",
    "print('The gauge Zeltweg (HZB 211136) has a mean waterlevel of', riverMean, 'cm, a median of', riverMedian, 'cm and a mode of', riverMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you might have made some changes to your variables `wellData` and `riverData`, it's probably a good idea to reimport the data, before you start the exercise. Also, the pandas documentation (especially the [user guide](http://pandas.pydata.org/pandas-docs/stable/user_guide/index.html)) is quite good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize these properties, please **plot two histograms for the data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use what you have learned so far (and probably the pandas documentation), to **find the years with the highes average water level** for the river gauge and the well.\n",
    "Getting the year as a nice, printable `int` is quite complicated, so for this exercise, just having the correct code without an nice, printed result is OK too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
